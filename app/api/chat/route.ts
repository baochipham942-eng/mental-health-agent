import { NextRequest, NextResponse } from 'next/server';
import { StreamData } from 'ai';
import { auth } from '@/auth'; // Adjust path if needed
import { prisma } from '@/lib/db/prisma';
import { quickAnalyze } from '@/lib/ai/groq';
import { streamCrisisReply } from '@/lib/ai/crisis';
import { streamSupportReply } from '@/lib/ai/support';
import { continueAssessment, streamAssessmentReply } from '@/lib/ai/assessment';
import { deepseek, streamEFTValidationReply } from '@/lib/ai/deepseek'; // Updated import
import { streamAssessmentConclusion } from '@/lib/ai/assessment/conclusion';
import { generateSFBTQuery } from '@/lib/ai/sfbt'; // SFBT logic
import { quickCrisisKeywordCheck } from '@/lib/ai/crisis-classifier';
import { ChatRequest, RouteType } from '@/types/chat';
import { memoryManager } from '@/lib/memory';
import { guardInput, getBlockedResponse } from '@/lib/ai/guardrails';
import { logInfo, logWarn, logError } from '@/lib/observability/logger';
import { analyzeRiskSignals, calculateTurn, inferPhase, shouldTriggerSafetyCheck } from '@/lib/ai/dialogue';
import { generateSummary, shouldSummarize, updateConversationSummary } from '@/lib/memory/summarizer';
import { analyzeConversationForStuckLoop, createStuckLoopEvent } from '@/lib/ai/detection/stuck-loop';
import { ChatService } from '@/lib/services/chat-service';

/**
 * Helper to create a stream response for fixed string content
 * Emulates the Vercel AI SDK protocol: 0:"text"\nd:{...}\n
 */
function createFixedStreamResponse(content: string, data: StreamData): NextResponse {
  const stream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder();
      controller.enqueue(encoder.encode(`0:${JSON.stringify(content)}\n`));
      data.close();
      const reader = data.stream.getReader();
      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          controller.enqueue(value);
        }
      } catch (e) {
        console.error('Error reading data stream', e);
      }
      controller.close();
    }
  });

  return new NextResponse(stream, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'X-Vercel-AI-Data-Stream': 'v1',
    },
  });
}

// =================================================================================
// é¢„è®¾æŠ€èƒ½å¡é…ç½® - ç”¨äºç›´æ¥æŠ€èƒ½è¯·æ±‚çš„å¿«é€Ÿå“åº”
// =================================================================================
// Export for testing
export const SKILL_CARDS = {
  breathing: {
    title: '4-7-8å‘¼å¸æ³•',
    when: 'æ€ç»ªçº·ä¹±æˆ–ç„¦è™‘æ—¶',
    effort: 'low' as const,
    widget: 'breathing',
    steps: [
      'æ‰¾ä¸€ä¸ªèˆ’é€‚çš„å§¿åŠ¿åå¥½',
      'ç”¨é¼»å­å¸æ°”4ç§’',
      'å±ä½å‘¼å¸7ç§’',
      'ç”¨å˜´ç¼“æ…¢å‘¼æ°”8ç§’',
      'é‡å¤3-4æ¬¡'
    ],
  },
  meditation: {
    title: '5åˆ†é’Ÿæ­£å¿µå†¥æƒ³',
    when: 'éœ€è¦æ”¾æ¾æˆ–ä¸“æ³¨æ—¶',
    effort: 'low' as const,
    widget: 'meditation',
    steps: [
      'æ‰¾ä¸€ä¸ªå®‰é™çš„åœ°æ–¹åä¸‹',
      'é—­ä¸Šçœ¼ç›ï¼Œä¸“æ³¨å‘¼å¸',
      'æ³¨æ„èº«ä½“çš„æ„Ÿå—',
      'å½“æ€ç»ªé£˜èµ°æ—¶ï¼Œæ¸©æŸ”åœ°æ‹‰å›',
      'ä¿æŒ5åˆ†é’Ÿ'
    ],
  },
  grounding: {
    title: 'äº”æ„Ÿç€é™†',
    when: 'æ„Ÿåˆ°ç„¦è™‘æˆ–ææ…Œæ—¶',
    effort: 'low' as const,
    widget: undefined,
    steps: [
      'è¯´å‡ºä½ èƒ½çœ‹åˆ°çš„5æ ·ä¸œè¥¿',
      'è¯´å‡ºä½ èƒ½æ‘¸åˆ°çš„4æ ·ä¸œè¥¿',
      'è¯´å‡ºä½ èƒ½å¬åˆ°çš„3ç§å£°éŸ³',
      'è¯´å‡ºä½ èƒ½é—»åˆ°çš„2ç§æ°”å‘³',
      'è¯´å‡ºä½ èƒ½å°åˆ°çš„1ç§å‘³é“'
    ],
  },
  reframing: {
    title: 'è®¤çŸ¥é‡æ„ç»ƒä¹ ',
    when: 'å½“ä¸‹é™·å…¥æ¶ˆæå¿µå¤´æ—¶',
    effort: 'medium' as const,
    widget: undefined,
    steps: [
      'è¯†åˆ«å½“ä¸‹çš„æ¶ˆæå¿µå¤´',
      'å¯»æ‰¾æ”¯æŒè¿™ä¸ªå¿µå¤´çš„è¯æ®',
      'å¯»æ‰¾åé©³è¿™ä¸ªå¿µå¤´çš„è¯æ®',
      'å°è¯•æå‡ºä¸€ä¸ªæ›´å¹³è¡¡ã€å®¢è§‚çš„è§†è§’',
    ],
  },
  activation: {
    title: 'è¡Œä¸ºæ¿€æ´»å°ä»»åŠ¡',
    when: 'æ„Ÿåˆ°åŠ¨åŠ›ä¸è¶³æˆ–æƒ…ç»ªä½è½æ—¶',
    effort: 'low' as const,
    widget: undefined,
    steps: [
      'é€‰æ‹©ä¸€ä»¶å¯ä»¥åœ¨5åˆ†é’Ÿå†…å®Œæˆçš„å°äº‹',
      'ç«‹å³å»åšï¼Œä¸è¦çº ç»“æ„Ÿå—',
      'å®Œæˆåç»™è‡ªå·±ä¸€ä¸ªå¾®å°çš„æ­£åé¦ˆ',
    ],
  },
  empty_chair: {
    title: 'ç©ºæ¤…å­å¯¹è¯ç»ƒä¹ ',
    when: 'æœ‰æœªè§£çš„å¿ƒç»“æˆ–å¼ºçƒˆå§”å±ˆæ—¶',
    effort: 'high' as const,
    widget: 'empty_chair',
    steps: [
      'è®¾å®šå¯¹é¢æ¤…å­ä¸Šåç€çš„äºº',
      'å°½æƒ…å®£æ³„ä½ çš„çœŸå®æ„Ÿå—',
      'äº’æ¢ä½ç½®ï¼Œä½“éªŒå¯¹æ–¹çš„è§†è§’',
      'é‡æ–°æ•´åˆä½ çš„æ„Ÿå—'
    ],
  },
  mood_tracker: {
    title: 'æƒ…ç»ªè®°å½•',
    when: 'æ„Ÿè§‰å¾ˆç³Ÿä½†ä¸æ¸…æ¥šåŸå› æ—¶',
    effort: 'low' as const,
    widget: 'mood_tracker',
    steps: [
      'åœä¸‹æ¥ï¼Œè§‰å¯Ÿå½“ä¸‹çš„æ„Ÿå—',
      'é€‰æ‹©ä¸€ä¸ªæœ€è´´åˆ‡çš„æƒ…ç»ªè¯',
      'è¯„ä¼°æƒ…ç»ªçš„å¼ºçƒˆç¨‹åº¦',
      'è®°å½•è§¦å‘æƒ…ç»ªçš„æƒ³æ³•æˆ–äº‹ä»¶'
    ],
  },
  leaves_stream: {
    title: 'æºªæµè½å¶',
    when: 'åå¤çº ç»“ã€è¢«å¿µå¤´å›°æ‰°æ—¶',
    effort: 'low' as const,
    widget: 'leaves_stream',
    steps: [] // Widget handles logical steps
  }
};

export type SkillType = keyof typeof SKILL_CARDS;

/**
 * æ£€æµ‹ç›´æ¥æŠ€èƒ½è¯·æ±‚ç±»å‹
 */
export function detectDirectSkillRequest(message: string): SkillType | null {
  const lowerMsg = message.toLowerCase();

  // å‘¼å¸æ³•
  if (/(è¿›è¡Œ|å¼€å§‹|æˆ‘è¦|åšä¸ª|ç»ƒä¹ |è¯•è¯•).*(å‘¼å¸|4.?7.?8|æ·±å‘¼å¸)/.test(lowerMsg)) return 'breathing';
  // å†¥æƒ³
  if (/(è¿›è¡Œ|å¼€å§‹|æˆ‘è¦|åšä¸ª|ç»ƒä¹ |è¯•è¯•).*(å†¥æƒ³|æ­£å¿µ|é™å¿ƒ)/.test(lowerMsg)) return 'meditation';
  // ç€é™†
  if (/(è¿›è¡Œ|å¼€å§‹|æˆ‘è¦|åšä¸ª|ç»ƒä¹ |è¯•è¯•).*(ç€é™†|5.?4.?3.?2.?1)/.test(lowerMsg)) return 'grounding';
  // é‡æ„
  if (/(è¿›è¡Œ|å¼€å§‹|æˆ‘è¦|åšä¸ª|ç»ƒä¹ |è¯•è¯•).*(è®¤çŸ¥é‡æ„|æƒ³æ³•æŒ‘æˆ˜)/.test(lowerMsg)) return 'reframing';
  // è¡Œä¸ºæ¿€æ´»
  if (/(è¿›è¡Œ|å¼€å§‹|æˆ‘è¦|åšä¸ª|ç»ƒä¹ |è¯•è¯•).*(è¡Œä¸ºæ¿€æ´»|è¡ŒåŠ¨ä»»åŠ¡)/.test(lowerMsg)) return 'activation';
  // ç©ºæ¤…å­
  if (/(è¿›è¡Œ|å¼€å§‹|æˆ‘è¦|åšä¸ª|ç»ƒä¹ |è¯•è¯•).*(ç©ºæ¤…å­|å¯¹è¯ç»ƒä¹ )/.test(lowerMsg)) return 'empty_chair';
  // æƒ…ç»ªè®°å½• - å¿…é¡»å¸¦æœ‰"è®°å½•"æˆ–"æ‰“å¡"ç­‰åŠ¨ä½œè¯
  if (/(è¿›è¡Œ|å¼€å§‹|æˆ‘è¦|åšä¸ª|ç»ƒä¹ |æ‰“å¡|è¯•è¯•).*(æƒ…ç»ªè®°å½•|è®°å½•å¿ƒæƒ…|å¿ƒæƒ…è®°å½•|å¿ƒæƒ…æ‰“å¡)/.test(lowerMsg)) return 'mood_tracker';
  // è„±é’©
  if (/(è¿›è¡Œ|å¼€å§‹|æˆ‘è¦|åšä¸ª|ç»ƒä¹ |è¯•è¯•).*(æƒ³æ³•è„±é’©|æºªæµè½å¶|è½å¶ç»ƒä¹ )/.test(lowerMsg)) return 'leaves_stream';

  // æåº¦å…·ä½“çš„æŒ‡ä»¤
  if (/^4.?7.?8$/.test(lowerMsg)) return 'breathing';
  if (/^5.?4.?3.?2.?1$/.test(lowerMsg)) return 'grounding';

  return null;
}

/**
 * åˆ›å»ºå¸¦æŠ€èƒ½å¡çš„å¿«é€Ÿæµå¼å“åº”ï¼ˆè·³è¿‡ DeepSeekï¼‰
 */
function createSkillCardStreamResponse(
  skillType: SkillType,
  data: StreamData,
  metadata: Record<string, any>
): NextResponse {
  const skill = SKILL_CARDS[skillType];
  const introMessages: Record<SkillType, string> = {
    breathing: 'æ²¡é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥å…³æ³¨å‘¼å¸ï¼Œè¿™èƒ½å¸®ä½ å¿«é€Ÿå¹³é™ä¸‹æ¥ã€‚è¯·å‡†å¤‡å¥½ï¼ŒéšèŠ‚å¥å¼€å§‹ï¼š',
    meditation: 'å¥½çš„ï¼Œæ‰¾ä¸€ä¸ªä¸å—æ‰“æ‰°çš„ç©ºé—´ï¼Œè®©æˆ‘ä»¬é€šè¿‡å†¥æƒ³æ‰¾å›å†…å¿ƒçš„å®é™ã€‚ç‚¹å‡»å¼€å§‹ï¼š',
    grounding: 'æ²¡å…³ç³»ï¼Œæˆ‘ä»¬å…ˆè¯•ç€å›åˆ°å½“ä¸‹ã€‚è¯·è·Ÿç€è¿™ä¸ªç€é™†ç»ƒä¹ çš„æŒ‡å¼•ï¼Œä¸€æ­¥æ­¥æ¥ï¼š',
    reframing: 'å½“å¿µå¤´è®©ä½ æ„Ÿåˆ°å›°æ‰°æ—¶ï¼Œæ¢ä¸ªè§†è§’æˆ–è®¸ä¼šæœ‰æ–°å‘ç°ã€‚è¯•è¯•è¿™ä¸ªè®¤çŸ¥é‡æ„ç»ƒä¹ ï¼š',
    activation: 'å¦‚æœæ„Ÿåˆ°æ²¡åŠ¨åŠ›ï¼Œæˆ‘ä»¬å…ˆé€šè¿‡ä¸€ä¸ªå°å°çš„è¡ŒåŠ¨æ¥æ‰“ç ´åƒµå±€ã€‚è¯·çœ‹ä¸‹é¢çš„ä»»åŠ¡å¡ç‰‡ï¼š',
    empty_chair: 'æœ‰äº›è¯æ†‹åœ¨å¿ƒé‡Œä¸€å®šå¾ˆéš¾å—å§ã€‚åœ¨â€œç©ºæ¤…å­â€é¢å‰ï¼Œä½ å¯ä»¥æ”¾å¿ƒåœ°æŠ’å‘å‡ºæ¥ã€‚å‡†å¤‡å¥½äº†å—ï¼Ÿ',
    mood_tracker: 'è®°å½•å’Œè§‰å¯Ÿæ˜¯æ„ˆåˆçš„å¼€å§‹ã€‚æˆ‘ä¸€ç›´åœ¨è¿™é‡Œé™ªç€ä½ ï¼Œå…ˆæ¥è®°å½•ä¸‹ä½ æ­¤åˆ»æœ€çœŸå®çš„æ„Ÿè§‰å§ï¼š',
    leaves_stream: 'æ„Ÿè§‰æ€ç»ªä¹±ç³Ÿç³Ÿçš„æ—¶å€™ï¼Œè¯•ç€æŠŠå®ƒä»¬çœ‹ä½œæºªæµä¸Šçš„è½å¶ã€‚è®©æˆ‘ä»¬å¼€å§‹è¿™ä¸ªç»ƒä¹ ï¼š',
  };

  const stream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder();

      // 1. å…ˆè¾“å‡ºç®€çŸ­æ–‡å­—
      const intro = introMessages[skillType];
      controller.enqueue(encoder.encode(`0:${JSON.stringify(intro)}\n`));

      // 2. æ·»åŠ å…ƒæ•°æ®ï¼ˆåŒ…å« actionCardsï¼‰
      data.append({
        ...metadata,
        routeType: 'support',
        actionCards: [skill],
        fastSkillResponse: true,
      } as any);

      data.close();
      const reader = data.stream.getReader();
      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          controller.enqueue(value);
        }
      } catch (e) {
        console.error('Error reading data stream', e);
      }
      controller.close();
    }
  });

  return new NextResponse(stream, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'X-Vercel-AI-Data-Stream': 'v1',
    },
  });
}


export const dynamic = 'force-dynamic';

export async function POST(request: NextRequest) {
  let finalSessionId: string | undefined;
  let finalUserId: string | undefined;
  let routeType: RouteType = 'support';
  const data = new StreamData();

  try {
    const body: ChatRequest = await request.json();
    const { message, history = [], state, assessmentStage, meta } = body;
    // data is already declared outside or at the start of POST.
    // Actually, I'll declare it here inside the try block to ensure it's available for all catch/finally.

    if (!message || message.trim().length === 0) {
      return NextResponse.json({ error: 'æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º' }, { status: 400 });
    }

    // =================================================================================
    // 0.0.5 FAST SKILL CARD PATH - æé€Ÿè·¯å¾„ï¼Œè·³è¿‡æ‰€æœ‰ LLM è°ƒç”¨
    // =================================================================================
    const directSkillType = detectDirectSkillRequest(message);
    if (directSkillType) {
      console.log('[API] FAST PATH: Direct skill request detected, bypassing all LLM calls:', directSkillType);
      const skill = SKILL_CARDS[directSkillType];
      const introMessages: Record<SkillType, string> = {
        breathing: 'æ²¡é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥å…³æ³¨å‘¼å¸ï¼Œè¿™èƒ½å¸®ä½ å¿«é€Ÿå¹³é™ä¸‹æ¥ã€‚è¯·å‡†å¤‡å¥½ï¼ŒéšèŠ‚å¥å¼€å§‹ï¼š',
        meditation: 'å¥½çš„ï¼Œæ‰¾ä¸€ä¸ªä¸å—æ‰“æ‰°çš„ç©ºé—´ï¼Œè®©æˆ‘ä»¬é€šè¿‡å†¥æƒ³æ‰¾å›å†…å¿ƒçš„å®é™ã€‚ç‚¹å‡»å¼€å§‹ï¼š',
        grounding: 'æ²¡å…³ç³»ï¼Œæˆ‘ä»¬å…ˆè¯•ç€å›åˆ°å½“ä¸‹ã€‚è¯·è·Ÿç€è¿™ä¸ªç€é™†ç»ƒä¹ çš„æŒ‡å¼•ï¼Œä¸€æ­¥æ­¥æ¥ï¼š',
        reframing: 'å½“å¿µå¤´è®©ä½ æ„Ÿåˆ°å›°æ‰°æ—¶ï¼Œæ¢ä¸ªè§†è§’æˆ–è®¸ä¼šæœ‰æ–°å‘ç°ã€‚è¯•è¯•è¿™ä¸ªè®¤çŸ¥é‡æ„ç»ƒä¹ ï¼š',
        activation: 'å¦‚æœæ„Ÿåˆ°æ²¡åŠ¨åŠ›ï¼Œæˆ‘ä»¬å…ˆé€šè¿‡ä¸€ä¸ªå°å°çš„è¡ŒåŠ¨æ¥æ‰“ç ´åƒµå±€ã€‚è¯·çœ‹ä¸‹é¢çš„ä»»åŠ¡å¡ç‰‡ï¼š',
        empty_chair: 'æœ‰äº›è¯æ†‹åœ¨å¿ƒé‡Œä¸€å®šå¾ˆéš¾å—å§ã€‚åœ¨â€œç©ºæ¤…å­â€é¢å‰ï¼Œä½ å¯ä»¥æ”¾å¿ƒåœ°æŠ’å‘å‡ºæ¥ã€‚å‡†å¤‡å¥½äº†å—ï¼Ÿ',
        mood_tracker: 'è®°å½•å’Œè§‰å¯Ÿæ˜¯æ„ˆåˆçš„å¼€å§‹ã€‚æˆ‘ä¸€ç›´åœ¨è¿™é‡Œé™ªç€ä½ ï¼Œå…ˆæ¥è®°å½•ä¸‹ä½ æ­¤åˆ»æœ€çœŸå®çš„æ„Ÿè§‰å§ï¼š',
        leaves_stream: 'æ„Ÿè§‰æ€ç»ªä¹±ç³Ÿç³Ÿçš„æ—¶å€™ï¼Œè¯•ç€æŠŠå®ƒä»¬çœ‹ä½œæºªæµä¸Šçš„è½å¶ã€‚è®©æˆ‘ä»¬å¼€å§‹è¿™ä¸ªç»ƒä¹ ï¼š',
      };

      // å¼‚æ­¥ä¿å­˜æ¶ˆæ¯ï¼ˆä¸é˜»å¡ï¼‰
      if (body.sessionId) {
        ChatService.saveAssistantMessage(body.sessionId, introMessages[directSkillType], {
          routeType: 'support', actionCards: [skill], fastSkillResponse: true
        });
      }

      return createSkillCardStreamResponse(directSkillType, data, {
        timestamp: new Date().toISOString(),
        emotion: { label: 'neutral', score: 5 },
        safety: { label: 'normal', score: 0, reasoning: 'æ£€æµ‹åˆ°æ˜ç¡®ç»ƒä¹ è¯·æ±‚ï¼Œæ­£åœ¨ä¸ºä½ å¼€å¯æé€Ÿå¼•å¯¼' },
      });
    }

    // =================================================================================
    // 0.1 Input Guardrail - è¾“å…¥å®‰å…¨æ£€æµ‹
    // =================================================================================
    const inputGuard = guardInput(message);
    if (!inputGuard.safe) {
      logWarn('input-guard-blocked', { reason: inputGuard.reason });
      const data = new StreamData();
      data.append({
        timestamp: new Date().toISOString(),
        routeType: 'support',
        guardBlocked: inputGuard.reason || 'unknown'
      } as Record<string, string>);
      return createFixedStreamResponse(getBlockedResponse(inputGuard.reason), data);
    }

    // =================================================================================
    // 0.2 Persistence Setup
    // =================================================================================
    const session = await auth();
    finalSessionId = body.sessionId;
    finalUserId = session?.user?.id;
    const sessionId = finalSessionId;
    const userId = finalUserId;

    // Import helper dynamically or at top? Top is better but for this refactor we assume top import added.
    // We will add the import in a separate block or assume it's available.
    // Wait, I need to add the import first.

    logInfo('chat-request', {
      hasSession: !!session,
      userId,
      sessionId: body.sessionId,
      messageLen: message.length
    });

    // Save User Message - å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡å“åº”
    if (sessionId && userId) {
      // Return promise to not block? The original code didn't await the IIFE.
      ChatService.saveUserMessage(sessionId, userId, message);
    }

    // Helper wrapper to match previous usage
    const saveAssistantMessage = async (content: string, meta?: Record<string, any>) => {
      if (sessionId) {
        await ChatService.saveAssistantMessage(sessionId, content, meta);
      }
    };

    // =================================================================================
    // 0.5 Memory Retrieval + Groq Analysis (å¹¶è¡Œæ‰§è¡Œï¼ŒèŠ‚çœ ~300ms)
    // =================================================================================
    let memoryContext = '';
    let processedHistory = history;

    // å¹¶è¡Œæ‰§è¡Œï¼šGroq åˆ†æ + è®°å¿†æ£€ç´¢
    // ä¼ å…¥æœ€è¿‘2æ¡å†å²è®°å½•ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¸®åŠ© Groq åˆ¤æ–­æ„å›¾ï¼ˆå¦‚å›ç­”è¯„ä¼°é—®é¢˜ vs åˆ‡æ¢è¯é¢˜ï¼‰
    const recentContext = history.slice(-2);
    const groqPromise = quickAnalyze(message, recentContext);

    const memoryPromise = (userId && history.length > 0)
      ? (async () => {
        try {
          // Phase 3: Get full memory object including raw memories array
          return await memoryManager.getMemoriesForContext(userId, message);
        } catch (e) {
          console.error('[Memory] Failed:', e);
        }
        return { contextString: '', memories: [] };
      })()
      : Promise.resolve({ contextString: '', memories: [] });

    // åŒæ—¶ç­‰å¾…ä¸¤ä¸ªç»“æœ (Groq ç°åœ¨åŒ…å« safety reasoning)
    const [analysis, retrievalResult] = await Promise.all([groqPromise, memoryPromise]);

    // Check if retrievalResult is string (old return) or object
    if (typeof retrievalResult === 'string') {
      memoryContext = retrievalResult;
    } else if (retrievalResult && typeof retrievalResult === 'object') {
      memoryContext = retrievalResult.contextString || '';
      // Phase 3 Active Push: Inject relevant memories into data stream
      if (retrievalResult.memories?.length > 0) {
        data.append({
          timestamp: new Date().toISOString(),
          relevantMemories: retrievalResult.memories.map((m: any) => ({
            id: m.id,
            content: m.content,
            topic: m.topic,
            sourceConvId: m.sourceConvId
          }))
        } as any);
      }
    }

    // æ„å»ºç»Ÿä¸€çš„ safety å¯¹è±¡ (ä» Groq åˆ†æç»“æœä¸­æå–)
    const safetyData = {
      label: analysis.safety,
      score: analysis.safety === 'crisis' ? 9 : analysis.safety === 'urgent' ? 6 : 1,
      reasoning: analysis.safetyReasoning,
    };

    // æ„å»ºå¯¹è¯çŠ¶æ€å¯¹è±¡
    const stateData = {
      reasoning: analysis.stateReasoning,
      route: analysis.route,
    };

    console.log('[Groq] Quick analysis result:', analysis);
    console.log('[Safety] Assessment:', safetyData);

    // å¦‚æœ Groq æ£€æµ‹åˆ°å±æœºï¼Œå¼ºåˆ¶åˆ‡æ¢åˆ°å±æœºè·¯ç”±
    if (analysis.safety === 'crisis') {
      console.log('[API] Groq detected crisis, overriding route');
      routeType = 'crisis';
    }



    // æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆå¯¹è¯æ‘˜è¦ (æ”¾åœ¨å¹¶è¡Œä¹‹åï¼Œå› ä¸ºä¾èµ– history)
    if (userId && sessionId && history.length > 0 && shouldSummarize(history.length)) {
      try {
        console.log('[Summarizer] History length exceeds threshold, generating summary...');
        const summary = await generateSummary(history);
        if (summary) {
          await updateConversationSummary(sessionId, summary);
          memoryContext += `\n\n### å¯¹è¯èƒŒæ™¯æ‘˜è¦\n${summary}\n`;
          processedHistory = history.slice(-8);
          console.log('[Summarizer] Summary generated and history trimmed.');
        }
      } catch (e) {
        console.error('[Summarizer] Failed:', e);
      }
    }

    // =================================================================================
    // 0.55 User Context Injection - å°†ç”¨æˆ·æ˜µç§°æ³¨å…¥ä¸Šä¸‹æ–‡ï¼Œè®© AI å¯ä»¥è‡ªç„¶ä½¿ç”¨
    // =================================================================================
    const userNickname = session?.user?.nickname;
    if (userNickname) {
      memoryContext += `\n\n**ç”¨æˆ·ä¿¡æ¯**ï¼šç”¨æˆ·æ˜µç§°ä¸ºã€Œ${userNickname}ã€ã€‚ä½ å¯ä»¥åœ¨åˆé€‚çš„æ—¶æœºï¼ˆå¦‚å¼€åœºé—®å€™ã€é¼“åŠ±è¯­å¥ï¼‰ä½¿ç”¨è¿™ä¸ªæ˜µç§°æ¥å¢åŠ äº²åˆ‡æ„Ÿï¼Œä½†ä¸è¦æ¯å¥éƒ½ç”¨ï¼Œä¿æŒè‡ªç„¶ã€‚`;
    }

    // const data = new StreamData(); // Moved up
    const traceMetadata = { sessionId, userId };

    const emotionObj = { label: analysis.emotion.label, score: analysis.emotion.score };
    routeType = analysis.route;

    // åå¤‡ï¼šå…³é”®è¯æ£€æµ‹å±æœºï¼ˆé˜²æ­¢å°æ¨¡å‹æ¼æ£€ï¼‰
    if (routeType !== 'crisis' && quickCrisisKeywordCheck(message)) {
      console.log('[API] Crisis keyword detected, overriding route');
      routeType = 'crisis';
    }

    // ç§»é™¤åŸæœ‰ç¡¬ç¼–ç çš„å…³é”®è¯å¼ºåˆ¶è·¯ç”±é€»è¾‘ï¼Œæ”¹ç”± Groq åˆ†ææ„å›¾

    // =================================================================================
    // 0.6 Dialogue State Tracking - å¯¹è¯çŠ¶æ€è¿½è¸ª
    // =================================================================================
    const conversationTurn = calculateTurn(history);
    const riskSignals = analyzeRiskSignals(message);
    const dialoguePhase = inferPhase(conversationTurn, riskSignals.shouldTriggerSafetyAssessment);
    const safetyCheck = shouldTriggerSafetyCheck(riskSignals, conversationTurn, emotionObj?.score);

    logInfo('dialogue-state', {
      turn: conversationTurn,
      phase: dialoguePhase,
      riskLevel: riskSignals.level,
      triggeredSignals: riskSignals.triggeredSignals.slice(0, 3),
      shouldTriggerSafety: safetyCheck.shouldTrigger,
    });

    // Append analysis and dialogue metadata to stream
    data.append({
      timestamp: new Date().toISOString(),
      safety: safetyData,
      state: stateData, // å¯¹è¯çŠ¶æ€æ¨ç†
      dialogue: {
        turn: conversationTurn,
        phase: dialoguePhase,
        riskLevel: riskSignals.level,
      },
    } as any);

    // Fix: Sticky Logic Removed
    // Previously we forced 'assessment' if state was 'awaiting_followup'.
    // Now we trust Groq's context-aware routing.
    // However, if the route IS 'assessment' and we are 'awaiting_followup', that's fine.
    // If Groq says 'support' but we are 'awaiting_followup' -> user likely changed topic -> We respect 'support'.


    // =================================================================================
    // 1. Crisis Handler (Highest Priority)
    // =================================================================================
    console.log('[API] Route decision:', { routeType, state, message: message.substring(0, 50) });
    if (state === 'in_crisis' || routeType === 'crisis') {
      // é€€å‡ºæœºåˆ¶ï¼š
      // 1. æ˜¾å¼çš„å®‰å…¨å£°æ˜ (æ­£åˆ™)
      // 2. Groq å®‰å…¨åˆ†æä¹Ÿè®¤ä¸ºæ˜¯ 'normal' (åŒé‡ç¡®è®¤)
      const isExplicitSafety = /æˆ‘æ²¡äº‹äº†|æ„Ÿè§‰å¥½å¤šäº†|å·²ç»ä¸å¤„åœ¨å±é™©ä¸­äº†|æ”¾å¿ƒå§|åˆ é™¤.*è®°å¿†|ä¸èŠäº†|æ¢ä¸ªè¯é¢˜/.test(message);
      const isAnalysedSafe = safetyData.label === 'normal';

      if (state === 'in_crisis' && (isExplicitSafety || isAnalysedSafe)) {
        console.log('[API] De-escalating crisis state based on validation:', { isExplicitSafety, isAnalysedSafe });
        // De-escalate
        data.append({ timestamp: new Date().toISOString(), routeType: 'support', state: 'normal', emotion: null });

        const onFinishWithMeta = async (text: string, toolCalls?: any[]) => {
          // Non-blocking save
          saveAssistantMessage(text, {
            toolCalls,
            safety: safetyData,
            state: stateData,
          }).catch(e => console.error('[DB] Failed to save assistant message:', e));

          // CRITICAL FIX: Ensure full reply is in the data stream final packet
          data.append({
            reply: text,
            toolCalls,
            safety: safetyData,
          } as any);
          data.close();
        };

        const result = await streamSupportReply(message, history, { onFinish: onFinishWithMeta, traceMetadata });
        return result.toDataStreamResponse({ data });
      }

      data.append({ timestamp: new Date().toISOString(), routeType: 'crisis', state: 'in_crisis', emotion: emotionObj });

      const onCrisisFinish = async (text: string, toolCalls?: any[]) => {
        // Non-blocking save
        saveAssistantMessage(text, {
          toolCalls,
          safety: safetyData,
          state: stateData,
        }).catch(e => console.error('[DB] Failed to save assistant message:', e));

        data.append({
          reply: text,
          toolCalls,
          safety: safetyData,
        } as any);
        data.close();
      }

      const result = await streamCrisisReply(message, history, state === 'in_crisis', { onFinish: onCrisisFinish, traceMetadata });
      return result.toDataStreamResponse({ data });
    }

    // =================================================================================
    // 1.5 EFT Validation Logic - (The "Heart" Phase)
    // ä¼˜å…ˆå¤„ç†é«˜æƒ…ç»ªå”¤èµ· (éå±æœºçŠ¶æ€ä¸‹)
    // =================================================================================
    if (analysis.needsValidation) {
      console.log('[API] EFT Validation triggered (High Emotion Score)');

      const onFinishWithMeta = async (text: string) => {
        // Non-blocking save
        saveAssistantMessage(text, {
          routeType: 'support',
          subRoute: 'eft_validation',
          safety: safetyData,
          state: stateData
        }).catch(e => console.error('[DB] Failed to save assistant message:', e));

        data.append({
          reply: text,
          routeType: 'support',
          safety: safetyData,
          isEFT: true
        } as any);
        data.close();
      };

      const result = await streamEFTValidationReply(message, processedHistory, {
        onFinish: onFinishWithMeta,
        traceMetadata
      });
      return result.toDataStreamResponse({ data });
    }

    // =================================================================================
    // 2. Support Handler (Positive / Venting / Neutral)
    // =================================================================================
    if (routeType === 'support') {
      // SFBT Logic Detection
      // SFBT Logic Detection
      let sfbtInstruction = undefined;
      // Match: "æˆ‘å®Œæˆäº†â€œäº”æ„Ÿç€é™†â€ç»ƒä¹ ï¼Œç°åœ¨æ„Ÿè§‰ï¼šğŸ™‚ (4åˆ†)"
      // Matches the format sent by ActionCardItem
      const sfbtMatch = message.match(/æˆ‘å®Œæˆäº†â€œ(.+)â€ç»ƒä¹ ï¼Œç°åœ¨æ„Ÿè§‰ï¼š.*\((\d+)åˆ†\)/);
      if (sfbtMatch) {
        const [_, exerciseName, scoreStr] = sfbtMatch;
        const postScore = parseInt(scoreStr);
        // preScore is unknown, so we rely on absolute postScore logic
        sfbtInstruction = generateSFBTQuery({ postScore, exerciseName });
        logInfo('sfbt-trigger', { exerciseName, postScore });
      }

      // ç§»é™¤æ‰‹åŠ¨æ³¨å…¥ actionCards çš„é€»è¾‘ï¼Œæ”¹ç”± LLM é€šè¿‡å·¥å…·è°ƒç”¨ (support.ts) è‡ªä¸»æ¨èï¼Œ
      // ä»è€Œç¡®ä¿æ¨èå‰ä¼šæœ‰å…±æƒ…è¯æœ¯ã€‚
      data.append({
        timestamp: new Date().toISOString(),
        routeType: 'support',
        state: 'normal',
        emotion: emotionObj,
      });

      const onFinishWithMeta = async (text: string, toolCalls?: any[]) => {
        // Non-blocking save
        saveAssistantMessage(text, {
          toolCalls,
          safety: safetyData,
          state: stateData
        }).catch(e => console.error('[DB] Failed to save assistant message:', e));

        data.append({
          reply: text,
          toolCalls,
          safety: safetyData,
        } as any);
        data.close();
      };

      const result = await streamSupportReply(message, processedHistory, {
        onFinish: onFinishWithMeta,
        traceMetadata,
        memoryContext,
        systemInstructionInjection: sfbtInstruction
      });
      // data.close() moved to onFinish
      return result.toDataStreamResponse({ data });
    }

    // =================================================================================
    // 3. Assessment Handler (Intake Loop -> Conclusion)
    // =================================================================================
    if (routeType === 'assessment') {
      // ç§»é™¤ assessment è·¯ç”±ä¸‹çš„ç¡¬ç¼–ç æŠ€èƒ½å¿«æ·è·¯å¾„

      // Call Assessment Loop with State Classifier (Streaming Version)
      const onAssessmentFinish = async (text: string, toolCalls?: any[]) => {
        // Determine if it's a conclusion based on tool calls
        const isConclusion = toolCalls?.some(tc => tc.function.name === 'finish_assessment') || false;

        // Non-blocking save
        saveAssistantMessage(text, {
          toolCalls,
          routeType: 'assessment',
          assessmentStage: isConclusion ? 'conclusion' : 'intake',
          safety: safetyData,
          state: stateData,
        }).catch(e => console.error('[DB] Failed to save assistant message:', e));

        // ğŸ”„ å¼‚æ­¥æ£€æµ‹æ­»å¾ªç¯ï¼ˆä¸é˜»å¡å“åº”ï¼‰
        if (!isConclusion && sessionId) {
          analyzeConversationForStuckLoop(sessionId).then(result => {
            if (result?.isStuck) {
              createStuckLoopEvent(sessionId, result);
            }
          }).catch(err => console.error('[StuckLoop] Detection failed:', err));
        }

        data.append({
          reply: text,
          toolCalls,
          routeType: 'assessment',
          assessmentStage: isConclusion ? 'conclusion' : 'intake',
          safety: safetyData,
        } as any);
        data.close();
      };

      // ğŸ”„ Special Case: If we are already in conclusion stage OR the classifier says we should conclude
      if (assessmentStage === 'conclusion') {
        const allUserMessages = history.filter(m => m.role === 'user').map(m => m.content);
        allUserMessages.push(message);
        const initialMsg = allUserMessages[0] || message;
        const followupStr = allUserMessages.slice(1).join('\n\n') || 'ï¼ˆæ— è¡¥å……å›ç­”ï¼‰';

        const onConclusionFinish = async (text: string, actionCards: any[]) => {
          // Non-blocking save
          saveAssistantMessage(text, {
            routeType: 'assessment',
            assessmentStage: 'conclusion',
            actionCards,
          }).catch(e => console.error('[DB] Failed to save assistant message:', e));

          data.append({
            reply: text,
            actionCards,
            routeType: 'assessment',
            assessmentStage: 'conclusion',
            safety: safetyData,
          } as any);
          data.close();
        };

        const conclusionResult = await streamAssessmentConclusion(initialMsg, followupStr, history, {
          traceMetadata,
          onFinish: onConclusionFinish
        });
        return conclusionResult.toDataStreamResponse({ data });
      }

      const assessmentResult = await streamAssessmentReply(message, processedHistory, {
        traceMetadata,
        memoryContext,
        onFinish: onAssessmentFinish
      });

      // Check if conclusion is needed (Dynamic)
      // Note: True streaming assessment means we might need to handle conclusion transition 
      // differently if we want to stream the conclusion REPORT immediately.
      // For now, keep it simple: Intake streams, then client sends another msg or tool triggers it.

      // If we are already heading for a conclusion (State classifier previously said so)
      // we might want to skip intake streaming and go straight to conclusion streaming.
      // But classifyDialogueState is currently non-streaming.

      return assessmentResult.toDataStreamResponse({ data });
    }

    // Fallback? Should cover all cases.
    await saveAssistantMessage("Unexpected error: No route matched.");
    return NextResponse.json({ error: 'Unexpected route match' }, { status: 500 });

  } catch (error: any) {
    console.error('Chat API Error:', error);
    return NextResponse.json({ error: error.message || 'Error processing request' }, { status: 500 });
  } finally {
    // =================================================================================
    // å¼‚æ­¥è§¦å‘è®°å¿†æå– - ä¸é˜»å¡å“åº”
    // =================================================================================
    // Session and userId are captured at the start of the try block
    if (finalSessionId && finalUserId) {
      const sessionId = finalSessionId;
      // ä½¿ç”¨ setImmediate æ¨¡æ‹Ÿæˆ–ç›´æ¥åœ¨ finally ä¸­å¼‚æ­¥æ‰§è¡Œ
      Promise.resolve().then(async () => {
        try {
          await memoryManager.processConversation(sessionId);
          console.log('[Memory] Async extraction completed for:', sessionId);
        } catch (e) {
          console.error('[Memory] Async extraction failed:', e);
        }
      });
    }
  }
}
