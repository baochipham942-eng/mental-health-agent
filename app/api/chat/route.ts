import { NextRequest, NextResponse } from 'next/server';
import { StreamData } from 'ai';
import { auth } from '@/auth'; // Adjust path if needed
import { prisma } from '@/lib/db/prisma';
import { quickAnalyze } from '@/lib/ai/groq';
import { streamCrisisReply } from '@/lib/ai/crisis';
import { streamSupportReply } from '@/lib/ai/support';
import { continueAssessment, streamAssessmentReply } from '@/lib/ai/assessment';
import { deepseek, streamEFTValidationReply } from '@/lib/ai/deepseek'; // Updated import
import { generateAssessmentConclusion, streamAssessmentConclusion } from '@/lib/ai/assessment/conclusion';
import { quickCrisisKeywordCheck } from '@/lib/ai/crisis-classifier';
import { ChatRequest, RouteType } from '@/types/chat';
import { memoryManager } from '@/lib/memory';
import { guardInput, getBlockedResponse } from '@/lib/ai/guardrails';
import { logInfo, logWarn, logError } from '@/lib/observability/logger';
import { analyzeRiskSignals, calculateTurn, inferPhase, shouldTriggerSafetyCheck } from '@/lib/ai/dialogue';
import { generateSummary, shouldSummarize, updateConversationSummary } from '@/lib/memory/summarizer';
import { analyzeConversationForStuckLoop, createStuckLoopEvent } from '@/lib/ai/detection/stuck-loop';
import { ChatService } from '@/lib/services/chat-service';

/**
 * Helper to create a stream response for fixed string content
 * Emulates the Vercel AI SDK protocol: 0:"text"\nd:{...}\n
 */
function createFixedStreamResponse(content: string, data: StreamData): NextResponse {
  const stream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder();
      controller.enqueue(encoder.encode(`0:${JSON.stringify(content)}\n`));
      data.close();
      const reader = data.stream.getReader();
      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          controller.enqueue(value);
        }
      } catch (e) {
        console.error('Error reading data stream', e);
      }
      controller.close();
    }
  });

  return new NextResponse(stream, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'X-Vercel-AI-Data-Stream': 'v1',
    },
  });
}

// =================================================================================
// é¢„è®¾æŠ€èƒ½å¡é…ç½® - ç”¨äºç›´æ¥æŠ€èƒ½è¯·æ±‚çš„å¿«é€Ÿå“åº”
// =================================================================================
// Export for testing
export const SKILL_CARDS = {
  breathing: {
    title: '4-7-8å‘¼å¸æ³•',
    when: 'æ€ç»ªçº·ä¹±æˆ–ç„¦è™‘æ—¶',
    effort: 'low' as const,
    widget: 'breathing',
    steps: [
      'æ‰¾ä¸€ä¸ªèˆ’é€‚çš„å§¿åŠ¿åå¥½',
      'ç”¨é¼»å­å¸æ°”4ç§’',
      'å±ä½å‘¼å¸7ç§’',
      'ç”¨å˜´ç¼“æ…¢å‘¼æ°”8ç§’',
      'é‡å¤3-4æ¬¡'
    ],
  },
  meditation: {
    title: '5åˆ†é’Ÿæ­£å¿µå†¥æƒ³',
    when: 'éœ€è¦æ”¾æ¾æˆ–ä¸“æ³¨æ—¶',
    effort: 'low' as const,
    widget: 'meditation',
    steps: [
      'æ‰¾ä¸€ä¸ªå®‰é™çš„åœ°æ–¹åä¸‹',
      'é—­ä¸Šçœ¼ç›ï¼Œä¸“æ³¨å‘¼å¸',
      'æ³¨æ„èº«ä½“çš„æ„Ÿå—',
      'å½“æ€ç»ªé£˜èµ°æ—¶ï¼Œæ¸©æŸ”åœ°æ‹‰å›',
      'ä¿æŒ5åˆ†é’Ÿ'
    ],
  },
  grounding: {
    title: '5-4-3-2-1ç€é™†æŠ€æœ¯',
    when: 'æ„Ÿåˆ°ç„¦è™‘æˆ–ææ…Œæ—¶',
    effort: 'low' as const,
    widget: undefined,
    steps: [
      'è¯´å‡ºä½ èƒ½çœ‹åˆ°çš„5æ ·ä¸œè¥¿',
      'è¯´å‡ºä½ èƒ½æ‘¸åˆ°çš„4æ ·ä¸œè¥¿',
      'è¯´å‡ºä½ èƒ½å¬åˆ°çš„3ç§å£°éŸ³',
      'è¯´å‡ºä½ èƒ½é—»åˆ°çš„2ç§æ°”å‘³',
      'è¯´å‡ºä½ èƒ½å°åˆ°çš„1ç§å‘³é“'
    ],
  },
  reframing: {
    title: 'è®¤çŸ¥é‡æ„ç»ƒä¹ ',
    when: 'å½“ä¸‹é™·å…¥æ¶ˆæå¿µå¤´æ—¶',
    effort: 'medium' as const,
    widget: undefined,
    steps: [
      'è¯†åˆ«å½“ä¸‹çš„æ¶ˆæå¿µå¤´',
      'å¯»æ‰¾æ”¯æŒè¿™ä¸ªå¿µå¤´çš„è¯æ®',
      'å¯»æ‰¾åé©³è¿™ä¸ªå¿µå¤´çš„è¯æ®',
      'å°è¯•æå‡ºä¸€ä¸ªæ›´å¹³è¡¡ã€å®¢è§‚çš„è§†è§’',
    ],
  },
  activation: {
    title: 'è¡Œä¸ºæ¿€æ´»å°ä»»åŠ¡',
    when: 'æ„Ÿåˆ°åŠ¨åŠ›ä¸è¶³æˆ–æƒ…ç»ªä½è½æ—¶',
    effort: 'low' as const,
    widget: undefined,
    steps: [
      'é€‰æ‹©ä¸€ä»¶å¯ä»¥åœ¨5åˆ†é’Ÿå†…å®Œæˆçš„å°äº‹',
      'ç«‹å³å»åšï¼Œä¸è¦çº ç»“æ„Ÿå—',
      'å®Œæˆåç»™è‡ªå·±ä¸€ä¸ªå¾®å°çš„æ­£åé¦ˆ',
    ],
  },
  empty_chair: {
    title: 'ç©ºæ¤…å­å¯¹è¯ç»ƒä¹ ',
    when: 'æœ‰æœªè§£çš„å¿ƒç»“æˆ–å¼ºçƒˆå§”å±ˆæ—¶',
    effort: 'high' as const,
    widget: 'empty_chair',
    steps: [
      'è®¾å®šå¯¹é¢æ¤…å­ä¸Šåç€çš„äºº',
      'å°½æƒ…å®£æ³„ä½ çš„çœŸå®æ„Ÿå—',
      'äº’æ¢ä½ç½®ï¼Œä½“éªŒå¯¹æ–¹çš„è§†è§’',
      'é‡æ–°æ•´åˆä½ çš„æ„Ÿå—'
    ],
  },
  mood_tracker: {
    title: 'æƒ…ç»ªè®°å½•',
    when: 'æ„Ÿè§‰å¾ˆç³Ÿä½†ä¸æ¸…æ¥šåŸå› æ—¶',
    effort: 'low' as const,
    widget: 'mood_tracker',
    steps: [
      'åœä¸‹æ¥ï¼Œè§‰å¯Ÿå½“ä¸‹çš„æ„Ÿå—',
      'é€‰æ‹©ä¸€ä¸ªæœ€è´´åˆ‡çš„æƒ…ç»ªè¯',
      'è¯„ä¼°æƒ…ç»ªçš„å¼ºçƒˆç¨‹åº¦',
      'è®°å½•è§¦å‘æƒ…ç»ªçš„æƒ³æ³•æˆ–äº‹ä»¶'
    ],
  },
};

export type SkillType = keyof typeof SKILL_CARDS;

/**
 * æ£€æµ‹ç›´æ¥æŠ€èƒ½è¯·æ±‚ç±»å‹
 */
export function detectDirectSkillRequest(message: string): SkillType | null {
  const lowerMsg = message.toLowerCase();
  if (/å‘¼å¸|4.?7.?8|æ·±å‘¼å¸/.test(lowerMsg)) return 'breathing';
  if (/å†¥æƒ³|æ­£å¿µ|é™å¿ƒ|meditation/.test(lowerMsg)) return 'meditation';
  if (/ç€é™†|5.?4.?3.?2.?1|grounding/.test(lowerMsg)) return 'grounding';
  if (/é‡æ„|æƒ³æ³•æŒ‘æˆ˜|è®¤çŸ¥/.test(lowerMsg)) return 'reframing';
  if (/è¡Œä¸ºæ¿€æ´»|æ´»åŠ¨|å°ä»»åŠ¡/.test(lowerMsg)) return 'activation';
  if (/ç©ºæ¤…å­|å¯¹è¯ç»ƒä¹ |å®£æ³„|å§”å±ˆ/.test(lowerMsg)) return 'empty_chair';
  if (/æƒ…ç»ªè®°å½•|å¿ƒæƒ…|è®°å½•æƒ…ç»ª/.test(lowerMsg)) return 'mood_tracker';
  return null;
}

/**
 * åˆ›å»ºå¸¦æŠ€èƒ½å¡çš„å¿«é€Ÿæµå¼å“åº”ï¼ˆè·³è¿‡ DeepSeekï¼‰
 */
function createSkillCardStreamResponse(
  skillType: SkillType,
  data: StreamData,
  metadata: Record<string, any>
): NextResponse {
  const skill = SKILL_CARDS[skillType];
  const introMessages: Record<SkillType, string> = {
    breathing: 'å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å‘¼å¸ç»ƒä¹ ã€‚ç‚¹å‡»ä¸‹æ–¹å¼€å§‹ï¼Œè·ŸéšèŠ‚å¥ä¸€èµ·åšï¼š',
    meditation: 'å¥½çš„ï¼Œè®©æˆ‘ä»¬ä¸€èµ·åšä¸ªç®€çŸ­çš„æ­£å¿µå†¥æƒ³ã€‚ç‚¹å‡»å¼€å§‹ï¼Œæ‰¾ä¸€ä¸ªå®‰é™çš„åœ°æ–¹ï¼š',
    grounding: 'å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸®åŠ©ä½ å›åˆ°å½“ä¸‹çš„ç€é™†æŠ€æœ¯ã€‚æŒ‰æ­¥éª¤è¯•è¯•çœ‹ï¼š',
    reframing: 'è¿™æ˜¯ä¸€ä¸ªè®¤çŸ¥é‡æ„ç»ƒä¹ ï¼Œå¯ä»¥å¸®åŠ©ä½ ä»ä¸åŒè§’åº¦çœ‹å¾…å½“ä¸‹çš„æ¶ˆæå¿µå¤´ï¼š',
    activation: 'è¿™æ˜¯ä¸€ä¸ªè¡Œä¸ºæ¿€æ´»å°ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡å¾®å°çš„è¡ŒåŠ¨æ¥æå‡ä½ çš„åŠ¨åŠ›å’Œæƒ…ç»ªï¼š',
    empty_chair: 'ç©ºæ¤…å­æŠ€æœ¯æ˜¯å¤„ç†æœªç«Ÿæƒ…æ„Ÿçš„å¼ºåŠ›å·¥å…·ã€‚å‡†å¤‡å¥½é¢å¯¹é‚£ä¸ªâ€œäººâ€äº†å—ï¼Ÿç‚¹å‡»ä¸‹æ–¹å¼€å§‹ï¼š',
    mood_tracker: 'è®°å½•æƒ…ç»ªæ˜¯è‡ªæˆ‘è§‰å¯Ÿçš„ç¬¬ä¸€æ­¥ã€‚æ¥è¯•è¯•è®°å½•ä¸‹ä½ æ­¤åˆ»çš„æ„Ÿå—ï¼š',
  };

  const stream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder();

      // 1. å…ˆè¾“å‡ºç®€çŸ­æ–‡å­—
      const intro = introMessages[skillType];
      controller.enqueue(encoder.encode(`0:${JSON.stringify(intro)}\n`));

      // 2. æ·»åŠ å…ƒæ•°æ®ï¼ˆåŒ…å« actionCardsï¼‰
      data.append({
        ...metadata,
        routeType: 'support',
        actionCards: [skill],
        fastSkillResponse: true,
      } as any);

      data.close();
      const reader = data.stream.getReader();
      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          controller.enqueue(value);
        }
      } catch (e) {
        console.error('Error reading data stream', e);
      }
      controller.close();
    }
  });

  return new NextResponse(stream, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'X-Vercel-AI-Data-Stream': 'v1',
    },
  });
}


export const dynamic = 'force-dynamic';

export async function POST(request: NextRequest) {
  let finalSessionId: string | undefined;
  let finalUserId: string | undefined;
  let routeType: RouteType = 'support'; // Top-level definition

  try {
    const body: ChatRequest = await request.json();
    const { message, history = [], state, assessmentStage, meta } = body;

    if (!message || message.trim().length === 0) {
      return NextResponse.json({ error: 'æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º' }, { status: 400 });
    }

    // =================================================================================
    // 0.0.5 FAST SKILL CARD PATH - æé€Ÿè·¯å¾„ï¼Œè·³è¿‡æ‰€æœ‰ LLM è°ƒç”¨
    // =================================================================================
    const directSkillType = detectDirectSkillRequest(message);
    if (directSkillType) {
      console.log('[API] FAST PATH: Direct skill request detected, bypassing all LLM calls:', directSkillType);
      const data = new StreamData();
      const skill = SKILL_CARDS[directSkillType];
      const introMessages: Record<SkillType, string> = {
        breathing: 'å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å‘¼å¸ç»ƒä¹ ã€‚ç‚¹å‡»ä¸‹æ–¹å¼€å§‹ï¼Œè·ŸéšèŠ‚å¥ä¸€èµ·åšï¼š',
        meditation: 'å¥½çš„ï¼Œè®©æˆ‘ä»¬ä¸€èµ·åšä¸ªç®€çŸ­çš„æ­£å¿µå†¥æƒ³ã€‚ç‚¹å‡»å¼€å§‹ï¼Œæ‰¾ä¸€ä¸ªå®‰é™çš„åœ°æ–¹ï¼š',
        grounding: 'å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸®åŠ©ä½ å›åˆ°å½“ä¸‹çš„ç€é™†æŠ€æœ¯ã€‚æŒ‰æ­¥éª¤è¯•è¯•çœ‹ï¼š',
        reframing: 'è¿™æ˜¯ä¸€ä¸ªè®¤çŸ¥é‡æ„ç»ƒä¹ ï¼Œå¯ä»¥å¸®åŠ©ä½ ä»ä¸åŒè§’åº¦çœ‹å¾…å½“ä¸‹çš„æ¶ˆæå¿µå¤´ï¼š',
        activation: 'è¿™æ˜¯ä¸€ä¸ªè¡Œä¸ºæ¿€æ´»å°ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡å¾®å°çš„è¡ŒåŠ¨æ¥æå‡ä½ çš„åŠ¨åŠ›å’Œæƒ…ç»ªï¼š',
        empty_chair: 'ç©ºæ¤…å­æŠ€æœ¯æ˜¯å¤„ç†æœªç«Ÿæƒ…æ„Ÿçš„å¼ºåŠ›å·¥å…·ã€‚å‡†å¤‡å¥½é¢å¯¹é‚£ä¸ªâ€œäººâ€äº†å—ï¼Ÿç‚¹å‡»ä¸‹æ–¹å¼€å§‹ï¼š',
        mood_tracker: 'è®°å½•æƒ…ç»ªæ˜¯è‡ªæˆ‘è§‰å¯Ÿçš„ç¬¬ä¸€æ­¥ã€‚æ¥è¯•è¯•è®°å½•ä¸‹ä½ æ­¤åˆ»çš„æ„Ÿå—ï¼š',
      };

      // å¼‚æ­¥ä¿å­˜æ¶ˆæ¯ï¼ˆä¸é˜»å¡ï¼‰
      if (body.sessionId) {
        ChatService.saveAssistantMessage(body.sessionId, introMessages[directSkillType], {
          routeType: 'support', actionCards: [skill], fastSkillResponse: true
        });
      }

      return createSkillCardStreamResponse(directSkillType, data, {
        timestamp: new Date().toISOString(),
        emotion: { label: 'neutral', score: 5 },
        safety: { label: 'normal', score: 0, reasoning: 'Fast skill path - no safety check needed' },
      });
    }

    // =================================================================================
    // 0.1 Input Guardrail - è¾“å…¥å®‰å…¨æ£€æµ‹
    // =================================================================================
    const inputGuard = guardInput(message);
    if (!inputGuard.safe) {
      logWarn('input-guard-blocked', { reason: inputGuard.reason });
      const data = new StreamData();
      data.append({
        timestamp: new Date().toISOString(),
        routeType: 'support',
        guardBlocked: inputGuard.reason || 'unknown'
      } as Record<string, string>);
      return createFixedStreamResponse(getBlockedResponse(inputGuard.reason), data);
    }

    // =================================================================================
    // 0.2 Persistence Setup
    // =================================================================================
    const session = await auth();
    finalSessionId = body.sessionId;
    finalUserId = session?.user?.id;
    const sessionId = finalSessionId;
    const userId = finalUserId;

    // Import helper dynamically or at top? Top is better but for this refactor we assume top import added.
    // We will add the import in a separate block or assume it's available.
    // Wait, I need to add the import first.

    logInfo('chat-request', {
      hasSession: !!session,
      userId,
      sessionId: body.sessionId,
      messageLen: message.length
    });

    // Save User Message - å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡å“åº”
    if (sessionId && userId) {
      // Return promise to not block? The original code didn't await the IIFE.
      ChatService.saveUserMessage(sessionId, userId, message);
    }

    // Helper wrapper to match previous usage
    const saveAssistantMessage = async (content: string, meta?: Record<string, any>) => {
      if (sessionId) {
        await ChatService.saveAssistantMessage(sessionId, content, meta);
      }
    };

    // =================================================================================
    // 0.5 Memory Retrieval + Groq Analysis (å¹¶è¡Œæ‰§è¡Œï¼ŒèŠ‚çœ ~300ms)
    // =================================================================================
    let memoryContext = '';
    let processedHistory = history;

    // å¹¶è¡Œæ‰§è¡Œï¼šGroq åˆ†æ + è®°å¿†æ£€ç´¢
    // ä¼ å…¥æœ€è¿‘2æ¡å†å²è®°å½•ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¸®åŠ© Groq åˆ¤æ–­æ„å›¾ï¼ˆå¦‚å›ç­”è¯„ä¼°é—®é¢˜ vs åˆ‡æ¢è¯é¢˜ï¼‰
    const recentContext = history.slice(-2);
    const groqPromise = quickAnalyze(message, recentContext);

    const memoryPromise = (userId && history.length > 0)
      ? (async () => {
        try {
          // Phase 3: Get full memory object including raw memories array
          return await memoryManager.getMemoriesForContext(userId, message);
        } catch (e) {
          console.error('[Memory] Failed:', e);
        }
        return { contextString: '', memories: [] };
      })()
      : Promise.resolve({ contextString: '', memories: [] });

    // åŒæ—¶ç­‰å¾…ä¸¤ä¸ªç»“æœ (Groq ç°åœ¨åŒ…å« safety reasoning)
    const [analysis, retrievalResult] = await Promise.all([groqPromise, memoryPromise]);

    // Check if retrievalResult is string (old return) or object
    if (typeof retrievalResult === 'string') {
      memoryContext = retrievalResult;
    } else if (retrievalResult && typeof retrievalResult === 'object') {
      memoryContext = retrievalResult.contextString || '';
      // Phase 3 Active Push: Inject relevant memories into data stream
      if (retrievalResult.memories?.length > 0) {
        data.append({
          timestamp: new Date().toISOString(),
          relevantMemories: retrievalResult.memories.map((m: any) => ({
            id: m.id,
            content: m.content,
            topic: m.topic,
            sourceConvId: m.sourceConvId
          }))
        } as any);
      }
    }

    // æ„å»ºç»Ÿä¸€çš„ safety å¯¹è±¡ (ä» Groq åˆ†æç»“æœä¸­æå–)
    const safetyData = {
      label: analysis.safety,
      score: analysis.safety === 'crisis' ? 9 : analysis.safety === 'urgent' ? 6 : 1,
      reasoning: analysis.safetyReasoning,
    };

    // æ„å»ºå¯¹è¯çŠ¶æ€å¯¹è±¡
    const stateData = {
      reasoning: analysis.stateReasoning,
      route: analysis.route,
    };

    console.log('[Groq] Quick analysis result:', analysis);
    console.log('[Safety] Assessment:', safetyData);

    // å¦‚æœ Groq æ£€æµ‹åˆ°å±æœºï¼Œå¼ºåˆ¶åˆ‡æ¢åˆ°å±æœºè·¯ç”±
    if (analysis.safety === 'crisis') {
      console.log('[API] Groq detected crisis, overriding route');
      routeType = 'crisis';
    }



    // æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆå¯¹è¯æ‘˜è¦ (æ”¾åœ¨å¹¶è¡Œä¹‹åï¼Œå› ä¸ºä¾èµ– history)
    if (userId && sessionId && history.length > 0 && shouldSummarize(history.length)) {
      try {
        console.log('[Summarizer] History length exceeds threshold, generating summary...');
        const summary = await generateSummary(history);
        if (summary) {
          await updateConversationSummary(sessionId, summary);
          memoryContext += `\n\n### å¯¹è¯èƒŒæ™¯æ‘˜è¦\n${summary}\n`;
          processedHistory = history.slice(-8);
          console.log('[Summarizer] Summary generated and history trimmed.');
        }
      } catch (e) {
        console.error('[Summarizer] Failed:', e);
      }
    }

    const data = new StreamData();
    const traceMetadata = { sessionId, userId };

    const emotionObj = { label: analysis.emotion.label, score: analysis.emotion.score };
    routeType = analysis.route;

    // åå¤‡ï¼šå…³é”®è¯æ£€æµ‹å±æœºï¼ˆé˜²æ­¢å°æ¨¡å‹æ¼æ£€ï¼‰
    if (routeType !== 'crisis' && quickCrisisKeywordCheck(message)) {
      console.log('[API] Crisis keyword detected, overriding route');
      routeType = 'crisis';
    }

    // æ—§é€»è¾‘é™çº§ï¼ˆç”¨äºä¸ç²¾ç¡®åŒ¹é…çš„æƒ…å†µï¼‰
    const skillKeywords = /åšä¸ªç»ƒä¹ |æƒ³è¯•è¯•|ç¼“è§£ç„¦è™‘|å­¦ä¹ æ”¾æ¾|æ”¾æ¾æŠ€å·§|æ”¾æ¾æ–¹æ³•/i;
    const wantsSkillCard = skillKeywords.test(message);
    if (wantsSkillCard) {
      console.log('[API] Skill keyword detected, forcing support route with action card.');
      routeType = 'support';
    }

    // =================================================================================
    // 0.6 Dialogue State Tracking - å¯¹è¯çŠ¶æ€è¿½è¸ª
    // =================================================================================
    const conversationTurn = calculateTurn(history);
    const riskSignals = analyzeRiskSignals(message);
    const dialoguePhase = inferPhase(conversationTurn, riskSignals.shouldTriggerSafetyAssessment);
    const safetyCheck = shouldTriggerSafetyCheck(riskSignals, conversationTurn, emotionObj?.score);

    logInfo('dialogue-state', {
      turn: conversationTurn,
      phase: dialoguePhase,
      riskLevel: riskSignals.level,
      triggeredSignals: riskSignals.triggeredSignals.slice(0, 3),
      shouldTriggerSafety: safetyCheck.shouldTrigger,
    });

    // Append analysis and dialogue metadata to stream
    data.append({
      timestamp: new Date().toISOString(),
      safety: safetyData,
      state: stateData, // å¯¹è¯çŠ¶æ€æ¨ç†
      dialogue: {
        turn: conversationTurn,
        phase: dialoguePhase,
        riskLevel: riskSignals.level,
      },
    } as any);

    // Fix: Sticky Logic Removed
    // Previously we forced 'assessment' if state was 'awaiting_followup'.
    // Now we trust Groq's context-aware routing.
    // However, if the route IS 'assessment' and we are 'awaiting_followup', that's fine.
    // If Groq says 'support' but we are 'awaiting_followup' -> user likely changed topic -> We respect 'support'.


    // =================================================================================
    // 1. Crisis Handler (Highest Priority)
    // =================================================================================
    console.log('[API] Route decision:', { routeType, state, message: message.substring(0, 50) });
    if (state === 'in_crisis' || routeType === 'crisis') {
      // é€€å‡ºæœºåˆ¶ï¼š
      // 1. æ˜¾å¼çš„å®‰å…¨å£°æ˜ (æ­£åˆ™)
      // 2. Groq å®‰å…¨åˆ†æä¹Ÿè®¤ä¸ºæ˜¯ 'normal' (åŒé‡ç¡®è®¤)
      const isExplicitSafety = /æˆ‘æ²¡äº‹äº†|æ„Ÿè§‰å¥½å¤šäº†|å·²ç»ä¸å¤„åœ¨å±é™©ä¸­äº†|æ”¾å¿ƒå§|åˆ é™¤.*è®°å¿†|ä¸èŠäº†|æ¢ä¸ªè¯é¢˜/.test(message);
      const isAnalysedSafe = safetyData.label === 'normal';

      if (state === 'in_crisis' && (isExplicitSafety || isAnalysedSafe)) {
        console.log('[API] De-escalating crisis state based on validation:', { isExplicitSafety, isAnalysedSafe });
        // De-escalate
        data.append({ timestamp: new Date().toISOString(), routeType: 'support', state: 'normal', emotion: null });

        const onFinishWithMeta = async (text: string, toolCalls?: any[]) => {
          // Non-blocking save
          saveAssistantMessage(text, {
            toolCalls,
            safety: safetyData,
            state: stateData,
          }).catch(e => console.error('[DB] Failed to save assistant message:', e));

          // CRITICAL FIX: Ensure full reply is in the data stream final packet
          data.append({
            reply: text,
            toolCalls,
            safety: safetyData,
          } as any);
          data.close();
        };

        const result = await streamSupportReply(message, history, { onFinish: onFinishWithMeta, traceMetadata });
        return result.toDataStreamResponse({ data });
      }

      data.append({ timestamp: new Date().toISOString(), routeType: 'crisis', state: 'in_crisis', emotion: emotionObj });

      const onCrisisFinish = async (text: string, toolCalls?: any[]) => {
        // Non-blocking save
        saveAssistantMessage(text, {
          toolCalls,
          safety: safetyData,
          state: stateData,
        }).catch(e => console.error('[DB] Failed to save assistant message:', e));

        data.append({
          reply: text,
          toolCalls,
          safety: safetyData,
        } as any);
        data.close();
      }

      const result = await streamCrisisReply(message, history, state === 'in_crisis', { onFinish: onCrisisFinish, traceMetadata });
      return result.toDataStreamResponse({ data });
    }

    // =================================================================================
    // 1.5 EFT Validation Logic - (The "Heart" Phase)
    // ä¼˜å…ˆå¤„ç†é«˜æƒ…ç»ªå”¤èµ· (éå±æœºçŠ¶æ€ä¸‹)
    // =================================================================================
    if (analysis.needsValidation) {
      console.log('[API] EFT Validation triggered (High Emotion Score)');

      const onFinishWithMeta = async (text: string) => {
        // Non-blocking save
        saveAssistantMessage(text, {
          routeType: 'support',
          subRoute: 'eft_validation',
          safety: safetyData,
          state: stateData
        }).catch(e => console.error('[DB] Failed to save assistant message:', e));

        data.append({
          reply: text,
          routeType: 'support',
          safety: safetyData,
          isEFT: true
        } as any);
        data.close();
      };

      const result = await streamEFTValidationReply(message, processedHistory, {
        onFinish: onFinishWithMeta,
        traceMetadata
      });
      return result.toDataStreamResponse({ data });
    }

    // =================================================================================
    // 2. Support Handler (Positive / Venting / Neutral)
    // =================================================================================
    if (routeType === 'support') {
      let actionCards: any[] | undefined;
      if (wantsSkillCard) {
        // æ ¹æ®å…·ä½“å…³é”®è¯é€‰æ‹©åˆé€‚çš„æŠ€èƒ½å¡ç‰‡
        if (/å†¥æƒ³|æ­£å¿µ/.test(message)) {
          // å†¥æƒ³ç›¸å…³å…³é”®è¯ -> æ­£å¿µå†¥æƒ³å¡ç‰‡
          actionCards = [SKILL_CARDS.meditation];
        } else if (/æƒ…ç»ª|è®°å½•/.test(message)) {
          // æƒ…ç»ªè®°å½•
          actionCards = [SKILL_CARDS.mood_tracker];
        } else if (/ç©ºæ¤…å­|å§”å±ˆ|å®£æ³„/.test(message)) {
          // ç©ºæ¤…å­
          actionCards = [SKILL_CARDS.empty_chair];
        } else {
          // é»˜è®¤ï¼šå‘¼å¸ç»ƒä¹ å¡ç‰‡
          actionCards = [SKILL_CARDS.breathing];
        }
      }

      // Force exit assessment if we were in it
      data.append({
        timestamp: new Date().toISOString(),
        routeType: 'support',
        state: 'normal',
        emotion: emotionObj,
        ...(actionCards && { actionCards }), // Inject skill cards
      });

      const onFinishWithMeta = async (text: string, toolCalls?: any[]) => {
        // Non-blocking save
        saveAssistantMessage(text, actionCards
          ? { routeType: 'support', actionCards, toolCalls, safety: safetyData, state: stateData }
          : { toolCalls, safety: safetyData, state: stateData }
        ).catch(e => console.error('[DB] Failed to save assistant message:', e));

        data.append({
          reply: text,
          toolCalls,
          safety: safetyData,
        } as any);
        data.close();
      };

      const result = await streamSupportReply(message, processedHistory, { onFinish: onFinishWithMeta, traceMetadata, memoryContext });
      // data.close() moved to onFinish
      return result.toDataStreamResponse({ data });
    }

    // =================================================================================
    // 3. Assessment Handler (Intake Loop -> Conclusion)
    // =================================================================================
    if (routeType === 'assessment') {
      // âš¡ Skill Card Shortcut: If user explicitly requests a skill, bypass assessment loop
      const skillKeywords = /å‘¼å¸ç»ƒä¹ |æ”¾æ¾æŠ€å·§|æ”¾æ¾æ–¹æ³•|åšä¸ªç»ƒä¹ |æƒ³è¯•è¯•|ç¼“è§£ç„¦è™‘|å­¦ä¹ æ”¾æ¾|å†¥æƒ³|æ­£å¿µ|ç€é™†æŠ€æœ¯/i;
      const wantsSkillCard = skillKeywords.test(message);
      console.log('[API] Assessment route - skillKeywords test:', { message, wantsSkillCard });

      if (wantsSkillCard) {
        // æ ¹æ®å…·ä½“å…³é”®è¯é€‰æ‹©åˆé€‚çš„æŠ€èƒ½å¡ç‰‡
        let skillCard;
        let skillReply;

        if (/å†¥æƒ³|æ­£å¿µ/.test(message)) {
          skillCard = {
            title: 'æ­£å¿µå†¥æƒ³',
            steps: ['æ‰¾ä¸€ä¸ªå®‰é™èˆ’é€‚çš„åœ°æ–¹åä¸‹', 'è½»è½»é—­ä¸Šçœ¼ç›ï¼Œæ”¾æ¾èº«ä½“', 'ä¸“æ³¨äºå‘¼å¸çš„æ„Ÿè§‰', 'å½“æ³¨æ„åŠ›é£˜èµ°æ—¶ï¼Œæ¸©æŸ”åœ°æ‹‰å›æ¥'],
            when: 'æƒ³è¦æ”¾æ¾å¿ƒæƒ…æˆ–æé«˜ä¸“æ³¨åŠ›æ—¶',
            effort: 'medium',
            widget: 'meditation',
          };
          skillReply = 'å¥½çš„ï¼Œæˆ‘ä»¬æ¥åšä¸€ä¸ªç®€å•çš„æ­£å¿µå†¥æƒ³ç»ƒä¹ ï¼Œå¸®åŠ©ä½ æ”¾æ¾èº«å¿ƒã€‚è¯·ç‚¹å‡»ä¸‹æ–¹çš„å¡ç‰‡å¼€å§‹ï¼š';
        } else {
          skillCard = {
            title: '4-7-8 å‘¼å¸æ³•',
            steps: ['å¸æ°” 4 ç§’', 'å±æ¯ 7 ç§’', 'å‘¼æ°” 8 ç§’', 'é‡å¤ 3-4 æ¬¡'],
            when: 'æ„Ÿåˆ°ç„¦è™‘æˆ–éœ€è¦å¿«é€Ÿæ”¾æ¾æ—¶',
            effort: 'low',
            widget: 'breathing',
          };
          skillReply = 'å¥½çš„ï¼Œæˆ‘ä»¬æ¥åšä¸€ä¸ªç®€å•çš„å‘¼å¸ç»ƒä¹ æ¥å¸®åŠ©ä½ æ”¾æ¾ã€‚è¯·ç‚¹å‡»ä¸‹æ–¹çš„å¡ç‰‡å¼€å§‹ï¼š';
        }

        // Save with metadata so actionCards persist across page refresh (Non-blocking)
        saveAssistantMessage(skillReply, {
          routeType: 'support',
          state: 'normal',
          actionCards: [skillCard],
          safety: safetyData,
        }).catch(e => console.error('[DB] Failed to save assistant message:', e));

        data.append({
          timestamp: new Date().toISOString(),
          routeType: 'support', // Switch to support mode
          state: 'normal',
          actionCards: [skillCard],
        });

        return createFixedStreamResponse(skillReply, data);
      }

      // Call Assessment Loop with State Classifier (Streaming Version)
      const onAssessmentFinish = async (text: string, toolCalls?: any[]) => {
        // Determine if it's a conclusion based on tool calls
        const isConclusion = toolCalls?.some(tc => tc.function.name === 'finish_assessment') || false;

        // Non-blocking save
        saveAssistantMessage(text, {
          toolCalls,
          routeType: 'assessment',
          assessmentStage: isConclusion ? 'conclusion' : 'intake',
          safety: safetyData,
          state: stateData,
        }).catch(e => console.error('[DB] Failed to save assistant message:', e));

        // ğŸ”„ å¼‚æ­¥æ£€æµ‹æ­»å¾ªç¯ï¼ˆä¸é˜»å¡å“åº”ï¼‰
        if (!isConclusion && sessionId) {
          analyzeConversationForStuckLoop(sessionId).then(result => {
            if (result?.isStuck) {
              createStuckLoopEvent(sessionId, result);
            }
          }).catch(err => console.error('[StuckLoop] Detection failed:', err));
        }

        data.append({
          reply: text,
          toolCalls,
          routeType: 'assessment',
          assessmentStage: isConclusion ? 'conclusion' : 'intake',
          safety: safetyData,
        } as any);
        data.close();
      };

      // ğŸ”„ Special Case: If we are already in conclusion stage OR the classifier says we should conclude
      if (assessmentStage === 'conclusion') {
        const allUserMessages = history.filter(m => m.role === 'user').map(m => m.content);
        allUserMessages.push(message);
        const initialMsg = allUserMessages[0] || message;
        const followupStr = allUserMessages.slice(1).join('\n\n') || 'ï¼ˆæ— è¡¥å……å›ç­”ï¼‰';

        const onConclusionFinish = async (text: string, actionCards: any[]) => {
          // Non-blocking save
          saveAssistantMessage(text, {
            routeType: 'assessment',
            assessmentStage: 'conclusion',
            actionCards,
          }).catch(e => console.error('[DB] Failed to save assistant message:', e));

          data.append({
            reply: text,
            actionCards,
            routeType: 'assessment',
            assessmentStage: 'conclusion',
            safety: safetyData,
          } as any);
          data.close();
        };

        const conclusionResult = await streamAssessmentConclusion(initialMsg, followupStr, history, {
          traceMetadata,
          onFinish: onConclusionFinish
        });
        return conclusionResult.toDataStreamResponse({ data });
      }

      const assessmentResult = await streamAssessmentReply(message, processedHistory, {
        traceMetadata,
        memoryContext,
        onFinish: onAssessmentFinish
      });

      // Check if conclusion is needed (Dynamic)
      // Note: True streaming assessment means we might need to handle conclusion transition 
      // differently if we want to stream the conclusion REPORT immediately.
      // For now, keep it simple: Intake streams, then client sends another msg or tool triggers it.

      // If we are already heading for a conclusion (State classifier previously said so)
      // we might want to skip intake streaming and go straight to conclusion streaming.
      // But classifyDialogueState is currently non-streaming.

      return assessmentResult.toDataStreamResponse({ data });
    }

    // Fallback? Should cover all cases.
    await saveAssistantMessage("Unexpected error: No route matched.");
    return NextResponse.json({ error: 'Unexpected route match' }, { status: 500 });

  } catch (error: any) {
    console.error('Chat API Error:', error);
    return NextResponse.json({ error: error.message || 'Error processing request' }, { status: 500 });
  } finally {
    // =================================================================================
    // å¼‚æ­¥è§¦å‘è®°å¿†æå– - ä¸é˜»å¡å“åº”
    // =================================================================================
    // Session and userId are captured at the start of the try block
    if (finalSessionId && finalUserId) {
      const sessionId = finalSessionId;
      // ä½¿ç”¨ setImmediate æ¨¡æ‹Ÿæˆ–ç›´æ¥åœ¨ finally ä¸­å¼‚æ­¥æ‰§è¡Œ
      Promise.resolve().then(async () => {
        try {
          await memoryManager.processConversation(sessionId);
          console.log('[Memory] Async extraction completed for:', sessionId);
        } catch (e) {
          console.error('[Memory] Async extraction failed:', e);
        }
      });
    }
  }
}
