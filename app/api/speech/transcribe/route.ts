import { NextRequest, NextResponse } from 'next/server';
import Groq from 'groq-sdk';

const GROQ_API_KEY = process.env.GROQ_API_KEY;

// ä¿ç•™ç™¾åº¦é…ç½®ä½œä¸ºé™çº§æ–¹æ¡ˆ
const BAIDU_APP_ID = process.env.BAIDU_SPEECH_APP_ID;
const BAIDU_API_KEY = process.env.BAIDU_SPEECH_API_KEY;
const BAIDU_SECRET_KEY = process.env.BAIDU_SPEECH_SECRET_KEY;

let accessToken: string | null = null;
let tokenExpiry: number = 0;

async function getBaiduAccessToken(): Promise<string> {
    if (accessToken && Date.now() < tokenExpiry) {
        return accessToken;
    }

    const url = `https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=${BAIDU_API_KEY}&client_secret=${BAIDU_SECRET_KEY}`;
    const response = await fetch(url, { method: 'POST' });

    if (!response.ok) {
        throw new Error('è·å–ç™¾åº¦ Token å¤±è´¥');
    }

    const data = await response.json();
    accessToken = data.access_token;
    tokenExpiry = Date.now() + (29 * 24 * 60 * 60 * 1000);

    return accessToken!;
}

/**
 * ä½¿ç”¨ Groq Whisper è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—
 * æ·»åŠ  prompt å‚æ•°å‡å°‘å¹»è§‰è¾“å‡º
 */
async function transcribeWithGroq(audioFile: File): Promise<string> {
    const groq = new Groq({ apiKey: GROQ_API_KEY });

    const transcription = await groq.audio.transcriptions.create({
        file: audioFile,
        model: 'whisper-large-v3-turbo',
        language: 'zh',
        response_format: 'text',
        // æ·»åŠ  prompt æ¥å¼•å¯¼æ¨¡å‹ï¼Œå‡å°‘å¹»è§‰
        // è¿™æ˜¯ OpenAI å®˜æ–¹æ¨èçš„è§£å†³å¹»è§‰é—®é¢˜çš„æ–¹æ³•
        prompt: 'è¿™æ˜¯ä¸€æ®µä¸­æ–‡å¿ƒç†å’¨è¯¢å¯¹è¯å½•éŸ³ã€‚ç”¨æˆ·æ­£åœ¨è¡¨è¾¾è‡ªå·±çš„æ„Ÿå—å’Œæƒ³æ³•ã€‚',
    });

    return typeof transcription === 'string' ? transcription : (transcription as any).text || '';
}

/**
 * ä½¿ç”¨ç™¾åº¦è¯­éŸ³è¯†åˆ«ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
 */
async function transcribeWithBaidu(audioFile: File): Promise<string> {
    const token = await getBaiduAccessToken();
    const arrayBuffer = await audioFile.arrayBuffer();
    const audioBase64 = Buffer.from(arrayBuffer).toString('base64');

    let format = 'wav';
    const mimeType = audioFile.type || audioFile.name;

    if (audioFile.name.endsWith('.pcm') || mimeType.includes('octet-stream')) {
        format = 'pcm';
    } else if (mimeType.includes('pcm')) {
        format = 'pcm';
    } else if (mimeType.includes('webm')) {
        format = 'wav';
    } else if (mimeType.includes('mp4') || mimeType.includes('m4a')) {
        format = 'm4a';
    } else if (mimeType.includes('amr')) {
        format = 'amr';
    }

    const baiduUrl = `https://vop.baidu.com/server_api`;
    const payload = {
        format: format,
        rate: 16000,
        dev_pid: 1936,
        channel: 1,
        cuid: 'mental_health_app_' + Date.now(),
        token: token,
        speech: audioBase64,
        len: arrayBuffer.byteLength,
    };

    const baiduResponse = await fetch(baiduUrl, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(payload),
    });

    const result = await baiduResponse.json();

    if (result.err_no !== 0) {
        const errorMessages: Record<number, string> = {
            3300: 'è¾“å…¥å‚æ•°ä¸æ­£ç¡®',
            3301: 'éŸ³é¢‘è´¨é‡è¿‡å·®',
            3302: 'é‰´æƒå¤±è´¥',
            3303: 'è¯­éŸ³æœåŠ¡å™¨åç«¯é—®é¢˜',
            3304: 'ç”¨æˆ·çš„è¯·æ±‚ QPS è¶…é™',
            3305: 'ç”¨æˆ·çš„æ—¥ pv è¶…é™',
            3307: 'è¯­éŸ³æœåŠ¡å™¨åç«¯è¯†åˆ«å‡ºé”™é—®é¢˜',
            3308: 'éŸ³é¢‘è¿‡é•¿',
            3309: 'éŸ³é¢‘æ•°æ®é—®é¢˜',
            3310: 'è¾“å…¥çš„éŸ³é¢‘æ–‡ä»¶è¿‡å¤§',
            3311: 'é‡‡æ ·ç‡ rate å‚æ•°ä¸åœ¨é€‰é¡¹é‡Œ',
            3312: 'éŸ³é¢‘æ ¼å¼ format å‚æ•°ä¸åœ¨é€‰é¡¹é‡Œ',
        };
        throw new Error(errorMessages[result.err_no] || `ç™¾åº¦é”™è¯¯ ${result.err_no}`);
    }

    return result.result?.[0]?.trim() || '';
}

/**
 * è¯­éŸ³è½¬æ–‡å­— API
 * ä¼˜å…ˆä½¿ç”¨ Groq Whisperï¼Œå¤±è´¥æ—¶é™çº§åˆ°ç™¾åº¦
 */
export async function POST(request: NextRequest) {
    try {
        const formData = await request.formData();
        const audioFile = formData.get('audio') as File;

        if (!audioFile) {
            console.error('[Speech API] No audio file provided');
            return NextResponse.json({ error: 'æœªæä¾›éŸ³é¢‘æ–‡ä»¶' }, { status: 400 });
        }

        console.log('[Speech API] Received audio:', {
            name: audioFile.name,
            type: audioFile.type,
            size: audioFile.size,
        });

        if (audioFile.size > 10 * 1024 * 1024) {
            return NextResponse.json({ error: 'éŸ³é¢‘æ–‡ä»¶è¿‡å¤§ï¼ˆæœ€å¤§ 10MBï¼‰' }, { status: 400 });
        }

        if (audioFile.size < 500) {
            return NextResponse.json({ error: 'å½•éŸ³æ—¶é—´å¤ªçŸ­ï¼Œè¯·è¯´è¯åå†æ¾æ‰‹' }, { status: 400 });
        }

        let text = '';

        // ä¼˜å…ˆä½¿ç”¨ Groq Whisper
        if (GROQ_API_KEY) {
            try {
                console.log('[Speech API] Using Groq Whisper...');
                const startTime = Date.now();
                text = await transcribeWithGroq(audioFile);
                console.log(`[Speech API] Groq completed in ${Date.now() - startTime}ms`);
            } catch (groqError) {
                console.error('[Speech API] Groq failed, falling back to Baidu:', groqError);
                // é™çº§åˆ°ç™¾åº¦
                if (BAIDU_API_KEY && BAIDU_SECRET_KEY) {
                    text = await transcribeWithBaidu(audioFile);
                } else {
                    throw groqError;
                }
            }
        } else if (BAIDU_API_KEY && BAIDU_SECRET_KEY) {
            // æ²¡æœ‰ Groqï¼Œç›´æ¥ç”¨ç™¾åº¦
            console.log('[Speech API] Using Baidu ASR...');
            text = await transcribeWithBaidu(audioFile);
        } else {
            return NextResponse.json({ error: 'æœªé…ç½®è¯­éŸ³è¯†åˆ«æœåŠ¡' }, { status: 500 });
        }

        if (!text) {
            return NextResponse.json({ error: 'æœªè¯†åˆ«åˆ°è¯­éŸ³å†…å®¹' }, { status: 200 });
        }

        // ğŸ›¡ï¸ Whisper å¹»è§‰è¿‡æ»¤å™¨
        // Whisper æ¨¡å‹åœ¨éŸ³é¢‘è¿‡çŸ­/é™éŸ³/æ¨¡ç³Šæ—¶ä¼šäº§ç”Ÿ"å¹»è§‰"è¾“å‡ºï¼Œå¸¸è§çš„åŒ…æ‹¬ï¼š
        // - YouTube/è§†é¢‘å¹³å°çš„è®¢é˜…æç¤ºè¯­
        // - é‡å¤çš„æ— æ„ä¹‰çŸ­è¯­
        // - æ˜æ˜¾ä¸ä¸Šä¸‹æ–‡æ— å…³çš„æ¨å¹¿è¯­å¥
        const HALLUCINATION_PATTERNS = [
            'è¯·ä¸åç‚¹èµ',
            'è®¢é˜…è½¬å‘',
            'æ‰“èµæ”¯æŒ',
            'æ˜é•œä¸ç‚¹ç‚¹',
            'ç‚¹èµè®¢é˜…',
            'æ„Ÿè°¢è§‚çœ‹',
            'è®°å¾—ç‚¹èµ',
            'ä¸€é”®ä¸‰è¿',
            'ç´ è´¨ä¸‰è¿',
            'é•¿æŒ‰ç‚¹èµ',
            'è°¢è°¢å¤§å®¶',
            'ä¸‹æœŸå†è§',
            'æˆ‘ä»¬ä¸‹æœŸè§',
            'æ¬¢è¿è®¢é˜…',
            'thanks for watching',
            'please subscribe',
            'like and subscribe',
        ];

        const lowerText = text.toLowerCase();
        const isHallucination = HALLUCINATION_PATTERNS.some(pattern =>
            lowerText.includes(pattern.toLowerCase())
        );

        if (isHallucination) {
            console.warn('[Speech API] Detected Whisper hallucination, ignoring:', text);
            return NextResponse.json({
                error: 'æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³ï¼Œè¯·é‡æ–°è¯´è¯',
                hallucination: true
            }, { status: 200 });
        }

        console.log('[Speech API] Transcribed:', text.substring(0, 50) + (text.length > 50 ? '...' : ''));
        return NextResponse.json({ text });

    } catch (error) {
        console.error('[Speech API] Error:', error);
        return NextResponse.json({
            error: error instanceof Error ? error.message : 'æœåŠ¡å™¨é”™è¯¯'
        }, { status: 500 });
    }
}
